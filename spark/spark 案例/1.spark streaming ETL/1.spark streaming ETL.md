### ETL介绍

ETL是将业务系统的数据经过抽取、清洗转换之后加载到数据仓库的过程，目的是将企业中的分散、零乱、标准不统一的数据整合到一起，为企业的决策提供分析依据。
ETL是BI项目重要的一个环节。
通常情况下，在BI项目中ETL会花掉整个项目至少1/3的时间,ETL设计的好坏直接关接到BI项目的成败。

(1)数据抽取与清洗

这一环节的主要工作是获取源系统的数据，并按照数据仓库的规则，进行数据加工，对不完整的数据、错误的数据、重复的数据进行处理，最终提取出我们想要的数据。

(2)数据转换

数据转化要解决两个问题。

一是数据不一致。具体包括数据类型不一致、数据格式不一致、编码不一致等。

二是数据粒度的转换。源系统和数据仓库对于业务的抽象粒度不一致。源系统是业务的生产者，故其对业务的抽象程度较低，换言之，源系统存储了很详细的明细数据。而数据仓库则是给业务人员或者数据从业者提供一个平台来分析数据的，故其对业务的抽象程度较高，存储的都是进过一定程度汇总的数据。

(3)数据加载

当前两个问题都解决完了之后，接下来就进行数据的加载，进行数据加载需要明确以下几个问题。

一是作业加载方式：全量加载还是增加更新。

二是任务触发方式：时间触发还是事件触发。如果是时间触发的话，还需要确定任务的上下游关系和依赖触发关系。

三是调度工具：采用什么调度工具，才能满足上面的需求呢

1.  ### 需求分析

    某票务系统，接收到的订单信息为（用户名，年龄，身份证号，车次）需要通过etl技术进行转换的输出格式为：（姓名，年龄，出生年月日，乘车地，车次），以便后续的分析使用。

    Spark streaming
    实时计算充当ETL角色，收集每10秒钟的数据，进行计算后结果存入到mysql库中，本案例中通过nc
    -l方法充当实时的源数据流入。

    （1）流式处理实时数据写入mysql库中

    （2）车次字段转换为发车所在地城市

    创建字典表（车次对应出发地，本案例中使用map集合方法）

    （3）提取身份证上年龄部分

### 3.原始数据 {#原始数据 .ListParagraph}

用户名，年龄，身份证号，车次

zs,21,12313219900202123,a01

ls,22,12311119800101123,a02

ww,28,11122219501212123,a03

Mysql:

\[root@h201 \~\]\# service mysqld start

mysql&gt; create user 'spark' identified by 'spark123';

mysql&gt; grant all privileges on \*.\* to spark@'%' identified by
'spark123' with grant option;

mysql&gt; grant all privileges on \*.\* to spark@h201 identified by
'spark123';

\[hadoop@h201 \~\]\$ mysql -h h201 -u spark -p

mysql&gt; create database spark1;

mysql&gt; use spark1;

mysql&gt; create table s1(name varchar(50),age varchar(10),birthDate
varchar(50),address varchar(50),trainnumber varchar(50));

拷贝mysql驱动包到spark下jars目录中

\[hadoop@h201 ff\]\$ cp mysql-connector-java-5.1.27.jar
/home/hadoop/spark-2.1.1-bin-hadoop2.7/jars/

### 4开发  {#开发 .ListParagraph}

**package** com.zr1\
\
**import** java.sql.DriverManager\
**import** org.apache.spark.SparkConf\
**import** org.apache.spark.streaming.{Seconds, StreamingContext}\
\
**object** sf2 {\
**def** main(args: Array\[String\]): Unit = {\
\
**val** conf = **new**
SparkConf().setAppName(**"sf2"**).setMaster(**"local\[2\]"**)\
**val** ssc = **new** StreamingContext(conf, *Seconds*(10))\
\
**val** lines = ssc.socketTextStream(**"localhost"**, 9999)\
**val** cc =
*Map*(**"a01"**-&gt;**"bj"**,**"a02"**-&gt;**"sh"**,**"a03"**-&gt;**"gz"**,**"a04"**-&gt;**"wh"**,**"a05"**-&gt;**"tj"**)\
**val** results = lines.map(a=&gt;{\
**val** ff = a.split(**","**)\
**val** name = ff(0)\
**val** age = ff(1)\
**val** c1 = ff(2).substring(6,14)\
**val** c2 = ff(3)\
**val** f11 = cc.getOrElse(c2,**"xxx"**)\
(name,age,c1,f11,c2)\
})\
\
results.foreachRDD(rdd =&gt;{\
rdd.foreachPartition(partition=&gt;{\
**val** connection = *createConnetion*()\
partition.foreach(pair =&gt;{\
**val** sql =**s"insert into s1(name,age,birthDate,address,trainnumber)
values('\$**{pair.\_1}**','\$**{pair.\_2}**','\$**{pair.\_3}**','\$**{pair.\_4}**','\$**{pair.\_5}**');"\
**connection.createStatement().execute(sql)\
})\
connection.close()\
})\
})\
ssc.start()\
ssc.awaitTermination()\
}\
\
**def** createConnetion() ={\
Class.*forName*(**"com.mysql.jdbc.Driver"**)\
DriverManager.*getConnection*(**"jdbc:mysql://h201:3306/spark1"**,**"spark"**,**"spark123"**)\
}\
\
}

### 5执行 {#执行 .ListParagraph}

打开nc窗口

\[hadoop@h201 \~\]\$ nc -l 9999

拷贝jar包到spark集群上

\[hadoop@h201 \~\]\$ cp /ff/sparkjar/sf2.jar /home/hadoop/qq/

执行

\[hadoop@h201 spark-2.1.1-bin-hadoop2.7\]\$ bin/spark-submit --class
"com.zr1.sf2" /home/hadoop/qq/sf2.jar

Nc窗口中输入，模拟外部写入数据

\[hadoop@h201 \~\]\$ nc -l 9999

zs,21,12313219900202123,a01

zs,21,12313219900202123,a01

ls,22,12311119800101123,a02

（10秒中后，查看mysql，因为开发中窗口间隔为10秒）

结果：

查看mysql中 s1表

mysql&gt; select \* from s1;

+------+------+-----------+---------+-------------+

| name | age | birthDate | address | trainnumber |

+------+------+-----------+---------+-------------+

| zs | 21 | 19900202 | bj | a01 |

| zs | 21 | 19900202 | bj | a01 |

| ls | 22 | 19800101 | sh | a02 |

+------+------+-----------+---------+-------------+

输入的数据etl转换后写入了mysql中s1表中
