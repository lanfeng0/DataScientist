1.  ### 需求分析

    某金融公司，需要做好风控，对那些信誉不好的用户，我们需要设置黑名单，只要是黑名单中的用户，我们就给过滤掉，禁止提供贷款服务。既然是实时，必然用到
    SparkStreaming，本案例中模拟采用 socketTextStream，结合 nc
    命令为进入的流式数据。

    案例流程：

<!-- -->

1.  模拟通过征信系统获取到黑名单列表

2.  socketTextStream做为输入的流式数据

3.  过滤掉黑名单中的用户

### 2.StreamingContext介绍

StreamingContext在内部会创建一个 SparkContext
对象（SparkContext是所有Spark应用的入口，在StreamingContext对象中可以这样访问：ssc.sparkContext）。

StreamingContext还有另一个构造参数，即：批次间隔，这个值的大小需要根据应用的具体需求和可用的集群资源来确定。

context对象创建后，你还需要如下步骤：

(1)创建DStream对象，并定义好输入数据源。

(2)基于数据源DStream定义好计算逻辑和输出。

(3)调用streamingContext.start() 启动接收并处理数据。

(4)调用streamingContext.awaitTermination()
等待流式处理结束（不管是手动结束，还是发生异常错误）

(5)你可以主动调用 streamingContext.stop() 来手动停止处理流程。

### 3.开发 {#开发 .ListParagraph}

**package** com.zr1\
\
**import** org.apache.spark.SparkConf\
**import** org.apache.spark.streaming.{Seconds, StreamingContext}\
\
**object** Sfilter1 {\
**def** main(args: Array\[String\]): Unit = {\
**val** sparkconf = **new**
SparkConf().setMaster(**"local\[2\]"**).setAppName(**"SparkFilter"**)\
**val** ssc = **new** StreamingContext(sparkconf,*Seconds*(10))\
\
*//模拟黑名单\
***val** blacks = *List*(**"zs"**, **"ls"**)\
**val** blacksRDD = ssc.sparkContext.parallelize(blacks)\
.map(x =&gt; (x, **true**))\
\
**val** lines = ssc.socketTextStream(**"localhost"**, 9999)\
\
**val** ads = lines.map(x =&gt; (x.split(**","**)(1), x))\
.transform(rdd =&gt; {\
rdd.leftOuterJoin(blacksRDD).filter(x =&gt;
x.\_2.\_2.getOrElse(**false**) != **true**).map(x =&gt; x.\_2.\_1)\
})\
ads.print()\
\
ssc.start()\
ssc.awaitTermination()\
}\
}

### 执行任务

新开启一个会话

\[hadoop@h201 \~\]\$ nc -l 9999

执行黑名单过滤任务

\[hadoop@h201 spark-2.1.1-bin-hadoop2.7\]\$ bin/spark-submit --class
"com.zr1.Sfilter1" /home/hadoop/qq/sfilter.jar

Nc窗口输入

\[hadoop@h201 \~\]\$ nc -l 9999

20200202,zs

**Streaming窗口没有输出，因为是黑名单用户**

再次输入

20200202,abc

**Streaming窗口输出，以为不是黑名单中的用户放行**
