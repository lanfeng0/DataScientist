1.什么是spark
=============

Hadoop常用于解决高吞吐、批量处理的业务场景，例如离线计算结果用于浏览量统计。如果需要实时查看浏览量统计信息，Hadoop显然不符合这样的要求。Spark通过内存计算能力极大地提高了大数据处理速度，满足了以上场景的需要。此外，Spark还支持SQL查询，流式计算，图计算，机器学习等。通过对Java、Python、Scala、R等语言的支持，极大地方便了用户的使用。

2.spark特点
===========

(1)快速处理能力。随着实时大数据应用越来越多，Hadoop作为离线的高吞吐、低响应框架已不能满足这类需求。Hadoop
MapReduce的Job将中间输出和结果存储在HDFS中，读写HDFS造成磁盘IO成为瓶颈。Spark允许将中间输出和结果存储在内存中，节省了大量的磁盘IO。同时Spark自身的DAG执行引擎也支持数据在内存中的计算。Spark官网声称性能比Hadoop快100倍，如图2-3所示。即便是内存不足需要磁盘IO，其速度也是Hadoop的10倍以上。

(2)易于使用。Spark现在支持Java、Scala、Python和R等语言编写应用程序，大大降低了使用者的门槛。自带了80多个高等级操作符，允许在Scala，Python，R的shell中进行交互式查询。

(3)支持查询。Spark支持SQL及Hive SQL对数据查询。

(4)支持流式计算。与MapReduce只能处理离线数据相比，Spark还支持实时的流计算。Spark依赖Spark
Streaming对数据进行实时的处理，其流式处理能力还要强于Storm。

(5)可用性高。Spark自身实现了Standalone部署模式，此模式下的Master可以有多个，解决了单点故障问题。此模式完全可以使用其他集群管理器替换，比如YARN、Mesos、EC2等。

(6)丰富的数据源支持。Spark除了可以访问操作系统自身的文件系统和HDFS，还可以访问Cassandra,
HBase, Hive,
Tachyon以及任何Hadoop的数据源。这极大地方便了已经使用HDFS、Hbase的用户顺利迁移到Spark。

3.spark 生态
============

![](./1.spark 原理/media/image1.PNG){width="3.6020494313210847in"
height="2.7743055555555554in"}

(1)Spark
Core：包含Spark的基本功能；尤其是定义RDD的API、操作以及这两者上的动作。其他Spark的库都是构建在RDD和Spark
Core之上的。

(2)Spark SQL：提供通过Apache
Hive的SQL变体Hive查询语言（HiveQL）与Spark进行交互的API。每个数据库表被当做一个RDD，(3)Spark
SQL查询被转换为Spark操作。

(4)Spark Streaming：对实时数据流进行处理和控制。Spark
Streaming允许程序能够像普通RDD一样处理实时数据

(5)MLib：一个常用机器学习的算法库，算法被实现为对RDD的Spark操作。这个库包含可扩展的学习算法，比如分类、回归等需要对大量数据集进行迭代的操作

(6)GraphX：控制图、并行图操作和计算的一组算法和工具的集合。GraphX扩展了RDD
API，包含控制图、创建子图、访问路径上所有顶点的操作

4.spark 基本架构
================

Spark架构采用了分布式计算中的Master-Slave模型，Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。

Master作为整个集群的控制器，负责整个集群的正常运行。

Worker相当于是计算节点，接收主节点命令与进行状态汇报。

Executor负责任务的执行。

Driver负责控制一个应用的执行。

Client作为用户的客户端负责提交应用。

![](./1.spark 原理/media/image2.png){width="4.717015529308837in"
height="2.5868055555555554in"}

(1)Driver程序在worker集群中某个节点，而非Master节点，但是这个节点由Master指定

(2)Driver程序占据Worker的资源

(3)cluster
mode下Master可以使用–supervise对Driver进行监控，如果Driver挂了可以自动重启

(4)cluster
mode下Master节点和Worker节点一般不在同一局域网，因此就无法将Jar包分发到各个Worker，所以cluster
mode要求必须提前把Jar包放到各个Worker几点对应的目录下面

5.spark 运行原理
================

![](./1.spark 原理/media/image3.png){width="4.606146106736658in"
height="3.5258694225721783in"}

(1)构建Spark Application的运行环境，启动SparkContext

(2)SparkContext向资源管理器（可以是Standalone，Mesos，Yarn）申请运行Executor资源，并启动StandaloneExecutorbackend

(3)Executor向SparkContext申请Task

(4)SparkContext将应用程序分发给Executor

(5)SparkContext构建成DAG图，将DAG图分解成Stage、将Taskset发送给Task
Scheduler，最后由Task Scheduler将Task发送给Executor运行

(6)Task在Executor上运行，运行完释放所有资源

6.spark与Hadoop对比
===================

Spark 是在借鉴了 MapReduce
之上发展而来的，继承了其分布式并行计算的优点并改进了 MapReduce
明显的缺陷:

(1)Spark 把中间数据放到内存中，迭代运算效率高。MapReduce
中计算结果需要落地，保存到磁盘上，这样势必会影响整体速度，而 Spark 支持
DAG
图的分布式并行计算的编程框架，减少了迭代过程中数据的落地，提高了处理效率。（延迟加载）

(2)Spark 容错性高。Spark 引进了弹性分布式数据集 RDD (Resilient
DistributedDataset)
的抽象，它是分布在一组节点中的只读对象集合，这些集合是弹性的，如果数据集一部分丢失，则可以根据“血统”（即允许基于数据衍生过程）对它们进行重建。另外在RDD
计算时可以通过 CheckPoint 来实现容错。

(3)Spark 更加通用。mapreduce 只提供了 Map 和 Reduce 两种操作，Spark
提供的数据集操作类型有很多，大致分为：Transformations 和 Actions
两大类。Transformations包括
Map、Filter、FlatMap、Sample、GroupByKey、ReduceByKey、Union、Join、Cogroup、MapValues、Sort
等多种操作类型，同时还提供 Count, Actions 包括 Collect、Reduce、Lookup
和 Save 等操作
