第十章：MapReduce实践\
 {#第十章mapreduce实践 .ListParagraph}
======================

实践一：倒排索引实践（多job串联）
=================================

需求
----

有大量的文本（文档、网页），需要建立搜索索引

（1）第一次预期输出结果

  -------------------
  icss--a.txt 3
  
  icss--b.txt 2
  
  icss--c.txt 2
  
  pingping--a.txt 1
  
  pingping--b.txt 3
  
  pingping--c.txt 1
  
  ss--a.txt 2
  
  ss--b.txt 1
  
  ss--c.txt 1
  -------------------

（2）第二次预期输出结果

  -------------------------------------------------
  icss c.txt--&gt;2 b.txt--&gt;2 a.txt--&gt;3
  
  pingping c.txt--&gt;1 b.txt--&gt;3 a.txt--&gt;1
  
  ss c.txt--&gt;1 b.txt--&gt;1 a.txt--&gt;2
  -------------------------------------------------

第一次处理
----------

### 编写OneIndexMapper

  -------------------------------------------------------------------------------------------
  package com.icss.mapreduce.index;
  
  import java.io.IOException;
  
  import org.apache.hadoop.io.IntWritable;
  
  import org.apache.hadoop.io.LongWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Mapper;
  
  import org.apache.hadoop.mapreduce.lib.input.FileSplit;
  
  public class OneIndexMapper extends Mapper&lt;LongWritable, Text, Text , IntWritable&gt;{
  
  String name;
  
  Text k = new Text();
  
  IntWritable v = new IntWritable();
  
  @Override
  
  protected void setup(Context context)
  
  throws IOException, InterruptedException {
  
  // 获取文件名称
  
  FileSplit split = (FileSplit) context.getInputSplit();
  
  name = split.getPath().getName();
  
  }
  
  @Override
  
  protected void map(LongWritable key, Text value, Context context)
  
  throws IOException, InterruptedException {
  
  // 1 获取1行
  
  String line = value.toString();
  
  // 2 切割
  
  String\[\] fields = line.split(" ");
  
  for (String word : fields) {
  
  // 3 拼接
  
  k.set(word+"--"+name);
  
  v.set(1);
  
  // 4 写出
  
  context.write(k, v);
  
  }
  
  }
  
  }
  -------------------------------------------------------------------------------------------

### 编写OneIndexReducer

  -------------------------------------------------------------------------------------------
  package com.icss.mapreduce.index;
  
  import java.io.IOException;
  
  import org.apache.hadoop.io.IntWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Reducer;
  
  public class OneIndexReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{
  
  @Override
  
  protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,
  
  Context context) throws IOException, InterruptedException {
  
  int count = 0;
  
  // 1 累加求和
  
  for(IntWritable value: values){
  
  count +=value.get();
  
  }
  
  // 2 写出
  
  context.write(key, new IntWritable(count));
  
  }
  
  }
  -------------------------------------------------------------------------------------------

### 编写OneIndexDriver

  -----------------------------------------------------------------------
  package com.icss.mapreduce.index;
  
  import org.apache.hadoop.conf.Configuration;
  
  import org.apache.hadoop.fs.Path;
  
  import org.apache.hadoop.io.IntWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Job;
  
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  
  public class OneIndexDriver {
  
  public static void main(String\[\] args) throws Exception {
  
  args = **new** String\[\] { "e:/input/inputoneindex", "e:/output5" };
  
  Configuration conf = new Configuration();
  
  Job job = Job.getInstance(conf);
  
  job.setJarByClass(OneIndexDriver.class);
  
  job.setMapperClass(OneIndexMapper.class);
  
  job.setReducerClass(OneIndexReducer.class);
  
  job.setMapOutputKeyClass(Text.class);
  
  job.setMapOutputValueClass(IntWritable.class);
  
  job.setOutputKeyClass(Text.class);
  
  job.setOutputValueClass(IntWritable.class);
  
  FileInputFormat.setInputPaths(job, new Path(args\[0\]));
  
  FileOutputFormat.setOutputPath(job, new Path(args\[1\]));
  
  job.waitForCompletion(true);
  
  }
  
  }
  -----------------------------------------------------------------------

### 查看第一次输出结果

  -------------------
  icss--a.txt 3
  
  icss--b.txt 2
  
  icss--c.txt 2
  
  pingping--a.txt 1
  
  pingping--b.txt 3
  
  pingping--c.txt 1
  
  ss--a.txt 2
  
  ss--b.txt 1
  
  ss--c.txt 1
  -------------------

第二次处理
----------

### 编写TwoIndexMapper

  -----------------------------------------------------------------------------------
  package com.icss.mapreduce.index;
  
  import java.io.IOException;
  
  import org.apache.hadoop.io.LongWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Mapper;
  
  public class TwoIndexMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{
  
  Text k = new Text();
  
  Text v = new Text();
  
  @Override
  
  protected void map(LongWritable key, Text value, Context context)
  
  throws IOException, InterruptedException {
  
  // 1 获取1行数据
  
  String line = value.toString();
  
  // 2用“--”切割
  
  String\[\] fields = line.split("--");
  
  k.set(fields\[0\]);
  
  v.set(fields\[1\]);
  
  // 3 输出数据
  
  context.write(k, v);
  
  }
  
  }
  -----------------------------------------------------------------------------------

### 编写TwoIndexReducer

  --------------------------------------------------------------------------------------------------------------------------
  package com.icss.mapreduce.index;
  
  import java.io.IOException;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Reducer;
  
  public class TwoIndexReducer extends Reducer&lt;Text, Text, Text, Text&gt; {
  
  @Override
  
  protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {
  
  // icss a.txt 3
  
  // icss b.txt 2
  
  // icss c.txt 2
  
  // icss c.txt--&gt;2 b.txt--&gt;2 a.txt--&gt;3
  
  StringBuilder sb = new StringBuilder();
  
  // 1 拼接
  
  for (Text value : values) {
  
  sb.append(value.toString().replace("\\t", "--&gt;") + "\\t");
  
  }
  
  // 2 写出
  
  context.write(key, new Text(sb.toString()));
  
  }
  
  }
  --------------------------------------------------------------------------------------------------------------------------

### 编写TwoIndexDriver

  -----------------------------------------------------------------------
  package com.icss.mapreduce.index;
  
  import org.apache.hadoop.conf.Configuration;
  
  import org.apache.hadoop.fs.Path;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Job;
  
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  
  public class TwoIndexDriver {
  
  public static void main(String\[\] args) throws Exception {
  
  args = **new** String\[\] { "e:/input/inputtwoindex", "e:/output6" };
  
  Configuration config = new Configuration();
  
  Job job = Job.getInstance(config);
  
  job.setJarByClass(TwoIndexDriver.class);
  
  job.setMapperClass(TwoIndexMapper.class);
  
  job.setReducerClass(TwoIndexReducer.class);
  
  job.setMapOutputKeyClass(Text.class);
  
  job.setMapOutputValueClass(Text.class);
  
  job.setOutputKeyClass(Text.class);
  
  job.setOutputValueClass(Text.class);
  
  FileInputFormat.setInputPaths(job, new Path(args\[0\]));
  
  FileOutputFormat.setOutputPath(job, new Path(args\[1\]));
  
  boolean result = job.waitForCompletion(true);
  
  System.exit(result?0:1);
  
  }
  
  }
  -----------------------------------------------------------------------

### 第二次查看最终结果

  -------------------------------------------------
  icss c.txt--&gt;2 b.txt--&gt;2 a.txt--&gt;3
  
  pingping c.txt--&gt;1 b.txt--&gt;3 a.txt--&gt;1
  
  ss c.txt--&gt;1 b.txt--&gt;1 a.txt--&gt;2
  -------------------------------------------------

实践二：找博客共同好友
======================

需求
----

以下是博客的好友列表数据，冒号前是一个用户，冒号后是该用户的所有好友（数据中的好友关系是单向的）

求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？

需求分析
--------

先求出A、B、C、….等是谁的好友

第一次输出结果

  ----------------------
  A I,K,C,B,G,F,H,O,D,
  
  B A,F,J,E,
  
  C A,E,B,H,F,G,K,
  
  D G,C,K,A,L,F,E,H,
  
  E G,M,L,H,A,F,B,D,
  
  F L,M,D,C,G,A,
  
  G M,
  
  H O,
  
  I O,C,
  
  J O,
  
  K B,
  
  L D,E,
  
  M E,F,
  
  O A,H,I,J,F,
  ----------------------

第二次输出结果

  ---------------
  A-B E C
  
  A-C D F
  
  A-D E F
  
  A-E D B C
  
  A-F O B C D E
  
  A-G F E C D
  
  A-H E C D O
  
  A-I O
  
  A-J O B
  
  A-K D C
  
  A-L F E D
  
  A-M E F
  
  B-C A
  
  B-D A E
  
  B-E C
  
  B-F E A C
  
  B-G C E A
  
  B-H A E C
  
  B-I A
  
  B-K C A
  
  B-L E
  
  B-M E
  
  B-O A
  
  C-D A F
  
  C-E D
  
  C-F D A
  
  C-G D F A
  
  C-H D A
  
  C-I A
  
  C-K A D
  
  C-L D F
  
  C-M F
  
  C-O I A
  
  D-E L
  
  D-F A E
  
  D-G E A F
  
  D-H A E
  
  D-I A
  
  D-K A
  
  D-L E F
  
  D-M F E
  
  D-O A
  
  E-F D M C B
  
  E-G C D
  
  E-H C D
  
  E-J B
  
  E-K C D
  
  E-L D
  
  F-G D C A E
  
  F-H A D O E C
  
  F-I O A
  
  F-J B O
  
  F-K D C A
  
  F-L E D
  
  F-M E
  
  F-O A
  
  G-H D C E A
  
  G-I A
  
  G-K D A C
  
  G-L D F E
  
  G-M E F
  
  G-O A
  
  H-I O A
  
  H-J O
  
  H-K A C D
  
  H-L D E
  
  H-M E
  
  H-O A
  
  I-J O
  
  I-K A
  
  I-O A
  
  K-L D
  
  K-O A
  
  L-M E F
  ---------------

代码实现：
----------

### 第一次Mapper 

  ----------------------------------------------------------------------------------------------------------------
  package com.icss.mapreduce.friends;
  
  import java.io.IOException;
  
  import org.apache.hadoop.io.LongWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Mapper;
  
  public class OneShareFriendsMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{
  
  @Override
  
  protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)
  
  throws IOException, InterruptedException {
  
  // 1 获取一行 A:B,C,D,F,E,O
  
  String line = value.toString();
  
  // 2 切割
  
  String\[\] fields = line.split(":");
  
  // 3 获取person和好友
  
  String person = fields\[0\];
  
  String\[\] friends = fields\[1\].split(",");
  
  // 4写出去
  
  for(String friend: friends){
  
  // 输出 &lt;好友，人&gt;
  
  context.write(new Text(friend), new Text(person));
  
  }
  
  }
  
  }
  ----------------------------------------------------------------------------------------------------------------

### 第一次Reducer

  ------------------------------------------------------------------------------------
  package com.icss.mapreduce.friends;
  
  import java.io.IOException;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Reducer;
  
  public class OneShareFriendsReducer extends Reducer&lt;Text, Text, Text, Text&gt;{
  
  @Override
  
  protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
  
  throws IOException, InterruptedException {
  
  StringBuffer sb = new StringBuffer();
  
  //1 拼接
  
  for(Text person: values){
  
  sb.append(person).append(",");
  
  }
  
  //2 写出
  
  context.write(key, new Text(sb.toString()));
  
  }
  
  }
  ------------------------------------------------------------------------------------

### 第一次Driver 

  -----------------------------------------------------------------
  package com.icss.mapreduce.friends;
  
  import org.apache.hadoop.conf.Configuration;
  
  import org.apache.hadoop.fs.Path;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Job;
  
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  
  public class OneShareFriendsDriver {
  
  public static void main(String\[\] args) throws Exception {
  
  // 1 获取job对象
  
  Configuration configuration = new Configuration();
  
  Job job = Job.getInstance(configuration);
  
  // 2 指定jar包运行的路径
  
  job.setJarByClass(OneShareFriendsDriver.class);
  
  // 3 指定map/reduce使用的类
  
  job.setMapperClass(OneShareFriendsMapper.class);
  
  job.setReducerClass(OneShareFriendsReducer.class);
  
  // 4 指定map输出的数据类型
  
  job.setMapOutputKeyClass(Text.class);
  
  job.setMapOutputValueClass(Text.class);
  
  // 5 指定最终输出的数据类型
  
  job.setOutputKeyClass(Text.class);
  
  job.setOutputValueClass(Text.class);
  
  // 6 指定job的输入原始所在目录
  
  FileInputFormat.setInputPaths(job, new Path(args\[0\]));
  
  FileOutputFormat.setOutputPath(job, new Path(args\[1\]));
  
  // 7 提交
  
  boolean result = job.waitForCompletion(true);
  
  System.exit(result?0:1);
  
  }
  
  }
  -----------------------------------------------------------------

### 第二次Mapper 

  ------------------------------------------------------------------------------------------
  package com.icss.mapreduce.friends;
  
  import java.io.IOException;
  
  import java.util.Arrays;
  
  import org.apache.hadoop.io.LongWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Mapper;
  
  public class TwoShareFriendsMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{
  
  @Override
  
  protected void map(LongWritable key, Text value, Context context)
  
  throws IOException, InterruptedException {
  
  // A I,K,C,B,G,F,H,O,D,
  
  // 友 人，人，人
  
  String line = value.toString();
  
  String\[\] friend\_persons = line.split("\\t");
  
  String friend = friend\_persons\[0\];
  
  String\[\] persons = friend\_persons\[1\].split(",");
  
  Arrays.sort(persons);
  
  for (int i = 0; i &lt; persons.length - 1; i++) {
  
  for (int j = i + 1; j &lt; persons.length; j++) {
  
  // 发出 &lt;人-人，好友&gt; ，这样，相同的“人-人”对的所有好友就会到同1个reduce中去
  
  context.write(new Text(persons\[i\] + "-" + persons\[j\]), new Text(friend));
  
  }
  
  }
  
  }
  
  }
  ------------------------------------------------------------------------------------------

### 第二次Reducer 

  ------------------------------------------------------------------------------------
  package com.icss.mapreduce.friends;
  
  import java.io.IOException;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Reducer;
  
  public class TwoShareFriendsReducer extends Reducer&lt;Text, Text, Text, Text&gt;{
  
  @Override
  
  protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
  
  throws IOException, InterruptedException {
  
  StringBuffer sb = new StringBuffer();
  
  for (Text friend : values) {
  
  sb.append(friend).append(" ");
  
  }
  
  context.write(key, new Text(sb.toString()));
  
  }
  
  }
  ------------------------------------------------------------------------------------

### 第二次Driver 

  -----------------------------------------------------------------
  package com.icss.mapreduce.friends;
  
  import org.apache.hadoop.conf.Configuration;
  
  import org.apache.hadoop.fs.Path;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Job;
  
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  
  public class TwoShareFriendsDriver {
  
  public static void main(String\[\] args) throws Exception {
  
  // 1 获取job对象
  
  Configuration configuration = new Configuration();
  
  Job job = Job.getInstance(configuration);
  
  // 2 指定jar包运行的路径
  
  job.setJarByClass(TwoShareFriendsDriver.class);
  
  // 3 指定map/reduce使用的类
  
  job.setMapperClass(TwoShareFriendsMapper.class);
  
  job.setReducerClass(TwoShareFriendsReducer.class);
  
  // 4 指定map输出的数据类型
  
  job.setMapOutputKeyClass(Text.class);
  
  job.setMapOutputValueClass(Text.class);
  
  // 5 指定最终输出的数据类型
  
  job.setOutputKeyClass(Text.class);
  
  job.setOutputValueClass(Text.class);
  
  // 6 指定job的输入原始所在目录
  
  FileInputFormat.setInputPaths(job, new Path(args\[0\]));
  
  FileOutputFormat.setOutputPath(job, new Path(args\[1\]));
  
  // 7 提交
  
  boolean result = job.waitForCompletion(true);
  
  System.exit(result?0:1);
  
  }
  
  }
  -----------------------------------------------------------------

实践三：TopN
============

需求
----

对前面输出结果进行加工，输出流量使用量在前10的用户信息

实现代码
--------

### 编写JavaBean类

  -------------------------------------------------------------------------
  > package com.icss.mr;
  >
  > import java.io.DataInput;
  >
  > import java.io.DataOutput;
  >
  > import java.io.IOException;
  >
  > import org.apache.hadoop.io.WritableComparable;
  >
  > public class FlowBean implements WritableComparable&lt;FlowBean&gt; {
  >
  > private Long sumFlow; // 总流量
  >
  > private String phoneNum; // 手机号
  >
  > // 空参构造
  >
  > public FlowBean() {
  >
  > super();
  >
  > }
  >
  > public Long getSumFlow() {
  >
  > return sumFlow;
  >
  > }
  >
  > public void setSumFlow(Long sumFlow) {
  >
  > this.sumFlow = sumFlow;
  >
  > }
  >
  > public String getPhoneNum() {
  >
  > return phoneNum;
  >
  > }
  >
  > public void setPhoneNum(String phoneNum) {
  >
  > this.phoneNum = phoneNum;
  >
  > }
  >
  > @Override
  >
  > public String toString() {
  >
  > return sumFlow + "\\t" + phoneNum;
  >
  > }
  >
  > @Override
  >
  > public void write(DataOutput out) throws IOException {
  >
  > // 序列化
  >
  > out.writeLong(sumFlow);
  >
  > out.writeUTF(phoneNum);
  >
  > }
  >
  > @Override
  >
  > public void readFields(DataInput in) throws IOException {
  >
  > // 反序列化
  >
  > this.sumFlow = in.readLong();
  >
  > this.phoneNum = in.readUTF();
  >
  > }
  >
  > @Override
  >
  > public int compareTo(FlowBean o) {
  >
  > int result;
  >
  > result = this.sumFlow.compareTo(o.sumFlow);
  >
  > if (result == 0) {
  >
  > result = this.phoneNum.compareTo(o.phoneNum);
  >
  > }
  >
  > return result;
  >
  > }
  >
  > }
  -------------------------------------------------------------------------

### 编写TopTenMapper类

  --------------------------------------------------------------------------------------------------------------
  package com.icss.mr;
  
  import java.io.IOException;
  
  import java.util.TreeMap;
  
  import org.apache.hadoop.io.LongWritable;
  
  import org.apache.hadoop.io.NullWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Mapper;
  
  public class TopTenMapper extends Mapper&lt;LongWritable, Text, NullWritable, Text&gt; {
  
  // 定义一个TreeMap作为存储数据的容器（天然按key排序）
  
  private TreeMap&lt;FlowBean, Text&gt; flowMap = new TreeMap&lt;FlowBean, Text&gt;();
  
  @Override
  
  protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
  
  FlowBean bean = new FlowBean();
  
  // 1.获取一行
  
  String line = value.toString();
  
  // 2.切割
  
  String\[\] fields = line.split("\\t");
  
  long sumFlow = Long.parseLong(fields\[3\]);
  
  bean.setSumFlow(sumFlow);
  
  bean.setPhoneNum(fields\[0\]);
  
  // 3.向TreeMap中添加数据
  
  flowMap.put(bean, new Text(value));
  
  // 4.限制TreeMap的数据量，超过10条就删除掉流量最小的一条数据
  
  if (flowMap.size() &gt; 10) {
  
  flowMap.remove(flowMap.firstKey());
  
  }
  
  }
  
  @Override
  
  protected void cleanup(Context context) throws IOException, InterruptedException {
  
  // 输出
  
  for (Text t : flowMap.values()) {
  
  context.write(NullWritable.get(), t);
  
  }
  
  }
  
  }
  --------------------------------------------------------------------------------------------------------------

### 编写TopTenReducer类

  --------------------------------------------------------------------------------------------
  package com.icss.mr;
  
  import java.io.IOException;
  
  import java.util.TreeMap;
  
  import org.apache.hadoop.io.NullWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Reducer;
  
  public class TopTenReducer extends Reducer&lt;NullWritable, Text, NullWritable, Text&gt; {
  
  @Override
  
  protected void reduce(NullWritable key, Iterable&lt;Text&gt; values, Context context)
  
  throws IOException, InterruptedException {
  
  // 1.定义一个TreeMap作为存储数据的容器（天然按key排序）
  
  TreeMap&lt;FlowBean, Text&gt; flowMap = new TreeMap&lt;FlowBean, Text&gt;();
  
  for (Text value : values) {
  
  FlowBean bean = new FlowBean();
  
  bean.setPhoneNum(value.toString().split("\\t")\[0\]);
  
  bean.setSumFlow(Long.parseLong(value.toString().split("\\t")\[3\]));
  
  flowMap.put(bean, new Text(value));
  
  // 2.限制TreeMap的数据量，超过10条就删除掉流量最小的一条数据
  
  if (flowMap.size() &gt; 10) {
  
  flowMap.remove(flowMap.firstKey());
  
  }
  
  }
  
  // 3.输出
  
  for (Text t : flowMap.descendingMap().values()) {
  
  context.write(NullWritable.get(), t);
  
  }
  
  }
  
  }
  --------------------------------------------------------------------------------------------

### 编写驱动类

  ----------------------------------------------------------------------------------------------
  package com.icss.mr;
  
  import java.io.IOException;
  
  import org.apache.hadoop.conf.Configuration;
  
  import org.apache.hadoop.fs.Path;
  
  import org.apache.hadoop.io.NullWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Job;
  
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  
  public class TopTenDriver {
  
  public static void main(String\[\] args)
  
  throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException {
  
  // 1 获取配置信息，或者job对象实例
  
  Configuration configuration = new Configuration();
  
  Job job = Job.*getInstance*(configuration);
  
  // 6 指定本程序的jar包所在的本地路径
  
  job.setJarByClass(TopTenDriver.class);
  
  // 2 指定本业务job要使用的*mapper*/Reducer业务类
  
  job.setMapperClass(TopTenMapper.class);
  
  job.setReducerClass(TopTenReducer.class);
  
  // 3 指定*mapper*输出数据的*kv*类型
  
  job.setMapOutputKeyClass(NullWritable.class);
  
  job.setMapOutputValueClass(Text.class);
  
  // 4 指定最终输出的数据的*kv*类型
  
  job.setOutputKeyClass(NullWritable.class);
  
  job.setOutputValueClass(Text.class);
  
  // 5 指定job的输入原始文件所在目录
  
  FileInputFormat.*setInputPaths*(job, new Path(args\[0\]));
  
  FileOutputFormat.*setOutputPath*(job, new Path(args\[1\]));
  
  // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行
  
  boolean result = job.waitForCompletion(true);
  
  System.*exit*(result ? 0 : 1);
  
  }
  
  }
  ----------------------------------------------------------------------------------------------

实践四：Reduce join 
====================

需求
----

订单数据表t\_order：

  ------ ----- --------
  id     pid   amount
  1001   01    1
  1002   02    2
  1003   03    3
  ------ ----- --------

商品信息表t\_product

  ----- -------
  pid   pname
  01    小米
  02    华为
  03    格力
  ----- -------

将商品信息表中数据根据商品pid合并到订单数据表中。

最终数据形式：

  ------ ------- --------
  id     pname   amount
  1001   小米    1
  1004   小米    4
  1002   华为    2
  1005   华为    5
  1003   格力    3
  1006   格力    6
  ------ ------- --------

通过将关联条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reduce
task，在reduce中进行数据的串联。

代码实现
--------

### 创建商品和订合并后的bean类

  -------------------------------------------------------------------------------------------
  package com.icss.mapreduce.table;
  
  import java.io.DataInput;
  
  import java.io.DataOutput;
  
  import java.io.IOException;
  
  import org.apache.hadoop.io.Writable;
  
  public class TableBean implements Writable {
  
  private String order\_id; // 订单id
  
  private String p\_id; // 产品id
  
  private int amount; // 产品数量
  
  private String pname; // 产品名称
  
  private String flag;// 表的标记
  
  public TableBean() {
  
  super();
  
  }
  
  public TableBean(String order\_id, String p\_id, int amount, String pname, String flag) {
  
  super();
  
  this.order\_id = order\_id;
  
  this.p\_id = p\_id;
  
  this.amount = amount;
  
  this.pname = pname;
  
  this.flag = flag;
  
  }
  
  public String getFlag() {
  
  return flag;
  
  }
  
  public void setFlag(String flag) {
  
  this.flag = flag;
  
  }
  
  public String getOrder\_id() {
  
  return order\_id;
  
  }
  
  public void setOrder\_id(String order\_id) {
  
  this.order\_id = order\_id;
  
  }
  
  public String getP\_id() {
  
  return p\_id;
  
  }
  
  public void setP\_id(String p\_id) {
  
  this.p\_id = p\_id;
  
  }
  
  public int getAmount() {
  
  return amount;
  
  }
  
  public void setAmount(int amount) {
  
  this.amount = amount;
  
  }
  
  public String getPname() {
  
  return pname;
  
  }
  
  public void setPname(String pname) {
  
  this.pname = pname;
  
  }
  
  @Override
  
  public void write(DataOutput out) throws IOException {
  
  out.writeUTF(order\_id);
  
  out.writeUTF(p\_id);
  
  out.writeInt(amount);
  
  out.writeUTF(pname);
  
  out.writeUTF(flag);
  
  }
  
  @Override
  
  public void readFields(DataInput in) throws IOException {
  
  this.order\_id = in.readUTF();
  
  this.p\_id = in.readUTF();
  
  this.amount = in.readInt();
  
  this.pname = in.readUTF();
  
  this.flag = in.readUTF();
  
  }
  
  @Override
  
  public String toString() {
  
  return order\_id + "\\t" + pname + "\\t" + amount + "\\t" ;
  
  }
  
  }
  -------------------------------------------------------------------------------------------

### 编写TableMapper程序

  -------------------------------------------------------------------------------------------------
  **package** com.icss.mapreduce.table;
  
  **import** java.io.IOException;
  
  **import** org.apache.hadoop.io.LongWritable;
  
  **import** org.apache.hadoop.io.Text;
  
  **import** org.apache.hadoop.mapreduce.Mapper;
  
  **import** org.apache.hadoop.mapreduce.lib.input.FileSplit;
  
  **public** **class** TableMapper **extends** Mapper&lt;LongWritable, Text, Text, TableBean&gt;{
  
  TableBean bean = **new** TableBean();
  
  Text k = **new** Text();
  
  @Override
  
  **protected** **void** map(LongWritable key, Text value, Context context)
  
  **throws** IOException, InterruptedException {
  
  // 1 获取输入文件类型
  
  FileSplit split = (FileSplit) context.getInputSplit();
  
  String name = split.getPath().getName();
  
  // 2 获取输入数据
  
  String line = value.toString();
  
  // 3 不同文件分别处理
  
  **if** (name.startsWith("order")) {// 订单表处理
  
  // 3.1 切割
  
  String\[\] fields = line.split("\\t");
  
  // 3.2 封装bean对象
  
  bean.setOrder\_id(fields\[0\]);
  
  bean.setP\_id(fields\[1\]);
  
  bean.setAmount(Integer.*parseInt*(fields\[2\]));
  
  bean.setPname("");
  
  bean.setFlag("0");
  
  k.set(fields\[1\]);
  
  }**else** {// 产品表处理
  
  // 3.3 切割
  
  String\[\] fields = line.split("\\t");
  
  // 3.4 封装bean对象
  
  bean.setP\_id(fields\[0\]);
  
  bean.setPname(fields\[1\]);
  
  bean.setFlag("1");
  
  bean.setAmount(0);
  
  bean.setOrder\_id("");
  
  k.set(fields\[0\]);
  
  }
  
  // 4 写出
  
  context.write(k, bean);
  
  }
  
  }
  -------------------------------------------------------------------------------------------------

### 编写TableReducer程序

  ---------------------------------------------------------------------------------------------
  package com.icss.mapreduce.table;
  
  import java.io.IOException;
  
  import java.util.ArrayList;
  
  import org.apache.commons.beanutils.BeanUtils;
  
  import org.apache.hadoop.io.NullWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Reducer;
  
  public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; {
  
  @Override
  
  protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context)
  
  throws IOException, InterruptedException {
  
  // 1准备存储订单的集合
  
  ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;();
  
  // 2 准备bean对象
  
  TableBean pdBean = new TableBean();
  
  for (TableBean bean : values) {
  
  if ("0".equals(bean.getFlag())) {// 订单表
  
  // 拷贝传递过来的每条订单数据到集合中
  
  TableBean orderBean = new TableBean();
  
  try {
  
  BeanUtils.copyProperties(orderBean, bean);
  
  } catch (Exception e) {
  
  e.printStackTrace();
  
  }
  
  orderBeans.add(orderBean);
  
  } else {// 产品表
  
  try {
  
  // 拷贝传递过来的产品表到内存中
  
  BeanUtils.copyProperties(pdBean, bean);
  
  } catch (Exception e) {
  
  e.printStackTrace();
  
  }
  
  }
  
  }
  
  // 3 表的拼接
  
  for(TableBean bean:orderBeans){
  
  bean.setPname (pdBean.getPname());
  
  // 4 数据写出去
  
  context.write(bean, NullWritable.get());
  
  }
  
  }
  
  }
  ---------------------------------------------------------------------------------------------

### 编写TableDriver程序

  -------------------------------------------------------------------------------
  package com.icss.mapreduce.table;
  
  import org.apache.hadoop.conf.Configuration;
  
  import org.apache.hadoop.fs.Path;
  
  import org.apache.hadoop.io.NullWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Job;
  
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  
  public class TableDriver {
  
  public static void main(String\[\] args) throws Exception {
  
  // 1 获取配置信息，或者job对象实例
  
  Configuration configuration = new Configuration();
  
  Job job = Job.getInstance(configuration);
  
  // 2 指定本程序的jar包所在的本地路径
  
  job.setJarByClass(TableDriver.class);
  
  // 3 指定本业务job要使用的mapper/Reducer业务类
  
  job.setMapperClass(TableMapper.class);
  
  job.setReducerClass(TableReducer.class);
  
  // 4 指定mapper输出数据的kv类型
  
  job.setMapOutputKeyClass(Text.class);
  
  job.setMapOutputValueClass(TableBean.class);
  
  // 5 指定最终输出的数据的kv类型
  
  job.setOutputKeyClass(TableBean.class);
  
  job.setOutputValueClass(NullWritable.class);
  
  // 6 指定job的输入原始文件所在目录
  
  FileInputFormat.setInputPaths(job, new Path(args\[0\]));
  
  FileOutputFormat.setOutputPath(job, new Path(args\[1\]));
  
  // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行
  
  boolean result = job.waitForCompletion(true);
  
  System.exit(result ? 0 : 1);
  
  }
  
  }
  -------------------------------------------------------------------------------

### 运行程序查看结果

  -------------
  1001 小米 1
  
  1001 小米 1
  
  1002 华为 2
  
  1002 华为 2
  
  1003 格力 3
  
  1003 格力 3
  -------------

缺点：这种方式中，合并的操作是在reduce阶段完成，reduce端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在reduce阶段极易产生数据倾斜

解决方案： map端实现数据合并

实践五：Map join
================

1）分析

适用于关联表中有小表的情形；

可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行合并并输出最终结果，可以大大提高合并操作的并发度，加快处理速度。

实现代码：
----------

### 先在驱动模块中添加缓存文件

  -----------------------------------------------------------------
  package test;
  
  import java.net.URI;
  
  import org.apache.hadoop.conf.Configuration;
  
  import org.apache.hadoop.fs.Path;
  
  import org.apache.hadoop.io.NullWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Job;
  
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  
  public class DistributedCacheDriver {
  
  public static void main(String\[\] args) throws Exception {
  
  // 1 获取job信息
  
  Configuration configuration = new Configuration();
  
  Job job = Job.getInstance(configuration);
  
  // 2 设置加载jar包路径
  
  job.setJarByClass(DistributedCacheDriver.class);
  
  // 3 关联map
  
  job.setMapperClass(DistributedCacheMapper.class);
  
  // 4 设置最终输出数据类型
  
  job.setOutputKeyClass(Text.class);
  
  job.setOutputValueClass(NullWritable.class);
  
  // 5 设置输入输出路径
  
  FileInputFormat.setInputPaths(job, new Path(args\[0\]));
  
  FileOutputFormat.setOutputPath(job, new Path(args\[1\]));
  
  // 6 加载缓存数据
  
  job.addCacheFile(**new** URI("file:///e:/tableinput/pd.txt"));
  
  // 7 map端join的逻辑不需要reduce阶段，设置reducetask数量为0
  
  job.setNumReduceTasks(0);
  
  // 8 提交
  
  boolean result = job.waitForCompletion(true);
  
  System.exit(result ? 0 : 1);
  
  }
  
  }
  -----------------------------------------------------------------

### 读取缓存的文件数据

  ---------------------------------------------------------------------------------------------------
  package test;
  
  import java.io.BufferedReader;
  
  import java.io.FileInputStream;
  
  import java.io.IOException;
  
  import java.io.InputStreamReader;
  
  import java.util.HashMap;
  
  import java.util.Map;
  
  import org.apache.commons.lang.StringUtils;
  
  import org.apache.hadoop.io.LongWritable;
  
  import org.apache.hadoop.io.NullWritable;
  
  import org.apache.hadoop.io.Text;
  
  import org.apache.hadoop.mapreduce.Mapper;
  
  public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;{
  
  Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;();
  
  @Override
  
  protected void setup(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)
  
  throws IOException, InterruptedException {
  
  // 1获取缓存文件
  
  URI\[\] cacheFiles = context.getCacheFiles();
  
  FileInputStream fis = new FileInputStream(cacheFiles\[0\].getPath().toString());
  
  InputStreamReader isr = new InputStreamReader(fis, "UTF-8");
  
  BufferedReader reader = new BufferedReader(isr);
  
  String line;
  
  while(StringUtils.isNotEmpty(line = reader.readLine())){
  
  // 2 切割
  
  String\[\] fields = line.split("\\t");
  
  // 3 缓存数据到集合
  
  pdMap.put(fields\[0\], fields\[1\]);
  
  }
  
  // 4 关流
  
  reader.close();
  
  }
  
  Text k = new Text();
  
  @Override
  
  protected void map(LongWritable key, Text value, Context context)
  
  throws IOException, InterruptedException {
  
  // 1 获取一行
  
  String line = value.toString();
  
  // 2 截取
  
  String\[\] fields = line.split("\\t");
  
  // 3 获取产品id
  
  String pId = fields\[1\];
  
  // 4 获取商品名称
  
  String pdName = pdMap.get(pId);
  
  // 5 拼接
  
  k.set(line + "\\t"+ pdName);
  
  // 6 写出
  
  context.write(k, NullWritable.get());
  
  }
  
  }
  ---------------------------------------------------------------------------------------------------


